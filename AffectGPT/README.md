<p align="center">
    <img src="assert/logo.png" width="150" style="margin-bottom: 0.2;"/>
<p>

<h3 align="center"><a href="https://openreview.net/pdf?id=xmbdACI0xu" style="color:#9C276A">
AffectGPT: A New Dataset, Model, and Benchmark
for Emotion Understanding with Multimodal Large
Language Models</a></h3>
<h5 align="center"> If our project helps you, please give us a star â­ on GitHub to support us. ğŸ™ğŸ™ </h2>

<h5 align="center">

</h5>

<img src="assert/demo-description.png" width="800" />

## ğŸ› ï¸ Requirements and Installation
My Dependencies (We have not tested other envs):
* python == 3.10
* pytorch == 2.4.0
* CUDA Version == 12.1
* transformers == 4.49.0
* tokenizers == 0.21.0
* vllm == 0.6.1

**[Environment Preparation]**
```bash
conda env create -f environment.yml
```

## ğŸš€ MER-Caption+

We construct a large-scale emotional description dataset MER-Caption, which adopts a model-led, human-assisted annotation strategy to strike a balance between label quality and dataset size.

<p><img src="assert/dataset.png" width="800" "/></p>

This dataset is available at: https://huggingface.co/datasets/MERChallenge/MER2025
```bash
dataset
â”œâ”€â”€ mer2025-dataset
|   â”œâ”€â”€ video # all training data, including 132,171 samples
|   â”œâ”€â”€ audio # pre-extracted audio
|   â”œâ”€â”€ openface_face # # pre-extracted face files
|   â”œâ”€â”€ subtitle_chieng.csv # pre-extracted subtitle content
|   â”œâ”€â”€ track2_train_mercaptionplus.csv # MER-Caption+ Dataset (OV labels)
|   â”œâ”€â”€ track3_train_mercaptionplus.csv # MER-Caption+ Dataset (Description)
```

## âœ¨ MER-UniBench
We build MER-UniBench, which encompasses typical MER tasks with tailored metrics. This benchmark can offer comprehensive evaluation results for MLLM-based emotion understanding.

```bash
## MER-UniBench includes 9 datasets
dataset 
# Available at: https://pan.baidu.com/s/1kbfs5pG_hAri0QwvQl-Ecg?pwd=b9vn
# Alternative link: https://1024terabox.com/s/1AE7uAU3Ib8aRBSyF1TMpow
â”œâ”€â”€ mer2023-dataset-process
â”œâ”€â”€ mer2024-dataset-process
â”œâ”€â”€ sims-process
â”œâ”€â”€ simsv2-process
â”œâ”€â”€ cmumosi-process
â”œâ”€â”€ cmumosei-process
â”œâ”€â”€ iemocap-process
â”œâ”€â”€ meld-process
# Available at: https://pan.baidu.com/s/1nBTw_ujSTQPAMyIs5Qv8Zw?pwd=k8tj
# Alternative link: https://1024terabox.com/s/1O130fc81FVsGGsrjLuHyDA
â”œâ”€â”€ ovmerdplus-process
```


## :earth_americas: Model Zoo
### General Checkpoints
| Model Name     | Model Type |
|:----------------|:------------:|
| [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)  | Visual Encoder  |
| [chinese-hubert-large](https://huggingface.co/TencentGameMate/chinese-hubert-large)  | Audio Encoder |
| [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)  | LLM |


### Pretrained AffectGPT Checkpoints
| Model Name     | Model Type |
|:----------------|:------------:|
| [emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz](https://pan.baidu.com/s/1wtKBxHQP4eCUSAVuBrOzag?pwd=27sh)  | Training on **MERCaption+** and take pre-extracted **face** as input  |

Meanwhile, we provide an alternate download link on Hugging Face: https://huggingface.co/MERChallenge/AffectGPT

## ğŸ—ï¸ Training & Inference

### Data and Pre-trained Checkpoints Structure
```bash
dataset
# MERCaption+ available at: https://huggingface.co/datasets/MERChallenge/MER2025
â”œâ”€â”€ mer2025-dataset
|   â”œâ”€â”€ video # all training data, including 132,171 samples
|   â”œâ”€â”€ audio # pre-extracted audio
|   â”œâ”€â”€ openface_face # # pre-extracted face files
|   â”œâ”€â”€ subtitle_chieng.csv # pre-extracted subtitle content
|   â”œâ”€â”€ track2_train_mercaptionplus.csv # MER-Caption+ Dataset (OV labels)
|   â”œâ”€â”€ track3_train_mercaptionplus.csv # MER-Caption+ Dataset (Description)

# MER-UniBench 
# Available at: https://pan.baidu.com/s/1kbfs5pG_hAri0QwvQl-Ecg?pwd=b9vn
# Alternative link: https://1024terabox.com/s/1AE7uAU3Ib8aRBSyF1TMpow
â”œâ”€â”€ mer2023-dataset-process
â”œâ”€â”€ mer2024-dataset-process
â”œâ”€â”€ sims-process
â”œâ”€â”€ simsv2-process
â”œâ”€â”€ cmumosi-process
â”œâ”€â”€ cmumosei-process
â”œâ”€â”€ iemocap-process
â”œâ”€â”€ meld-process
# Available at: https://pan.baidu.com/s/1nBTw_ujSTQPAMyIs5Qv8Zw?pwd=k8tj
# Alternative link: https://1024terabox.com/s/1O130fc81FVsGGsrjLuHyDA
â”œâ”€â”€ ovmerdplus-process

AffectGPT
â”œâ”€â”€ models # Available at: https://pan.baidu.com/s/1IvC4H7Xt1AzMFocGMBBbHQ?pwd=hzf9
â”‚   â”œâ”€â”€ chinese-hubert-large # audio encoders
â”‚   â”œâ”€â”€ clip-vit-large-patch14 # video encoders
â”‚   â”œâ”€â”€ Qwen2.5-7B-Instruct # LLM
```

### Training
```bash
CUDA_VISIBLE_DEVICES=0 python -u train.py 
--cfg-path=train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz.yaml
```

### Inference Code for MER-UniBench
1. Pre-trained Checkpoints Structure

We also provide pre-trained weights if you do not want to do the above training.
```bash
AffectGPT
â”œâ”€â”€ models # Available at: https://pan.baidu.com/s/1IvC4H7Xt1AzMFocGMBBbHQ?pwd=hzf9
â”‚   â”œâ”€â”€ chinese-hubert-large # audio encoders
â”‚   â”œâ”€â”€ clip-vit-large-patch14 # video encoders
â”‚   â”œâ”€â”€ Qwen2.5-7B-Instruct # LLM
â”œâ”€â”€ output # Available at: https://pan.baidu.com/s/1wtKBxHQP4eCUSAVuBrOzag?pwd=27sh
â”‚   â”œâ”€â”€ emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz # Training on mercaptionplus + input face
```

2. Inference Process 
```bash
# Prompt1: Generate OV labels
CUDA_VISIBLE_DEVICES=0 python -u inference_hybird.py --zeroshot --dataset='inferenceData' 
--cfg-path=train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz.yaml 
--options "inference.test_epochs=30-60" "inference.skip_epoch=5"

# Prompt2: Generate Emotion Description
CUDA_VISIBLE_DEVICES=0 python -u inference_hybird.py --zeroshot --dataset='inferenceData' 
--outside_user_message="Please infer the person's emotional state and provide your reasoning process."
--cfg-path=train_configs/emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz.yaml 
--options "inference.test_epochs=30-60" "inference.skip_epoch=5" "inference.base_root=output/results-description"
```


## ğŸ‘ Score Calculation

1. If you want to skip the above inference process, we also provide intermedia outputs for evaluation.
```bash
[1] prepare results
AffectGPT
â”œâ”€â”€ models # Available at: https://pan.baidu.com/s/1IvC4H7Xt1AzMFocGMBBbHQ?pwd=hzf9
â”‚   â”œâ”€â”€ chinese-hubert-large # audio encoders
â”‚   â”œâ”€â”€ clip-vit-large-patch14 # video encoders
â”‚   â”œâ”€â”€ Qwen2.5-7B-Instruct # LLM
â”œâ”€â”€ output # Available at: https://pan.baidu.com/s/1pIoMM3RT7BqeyGI0t5KdoQ?pwd=c7x2
â”‚   â”œâ”€â”€ emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz # Training on mercaptionplus + input face
â”‚   â”œâ”€â”€ **results-cmumosei** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-cmumosi** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-iemocapfour** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-meld** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-mer2023** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-mer2024** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-sims** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-simsv2** # Intermedia Outputs
â”‚   â”œâ”€â”€ **results-ovmerdplus** # Intermedia Outputs

[2] score calculation: ov labels + gt labels => score
CUDA_VISIBLE_DEVICES=0 python evaluation-scoreonly.py
```

2. Full evaluation code
```bash
CUDA_VISIBLE_DEVICES=0 python evaluation.py
```

<p><img src="assert/main-result.png" width="800" "/></p>



## ğŸ¤– Inference for Single Video
1. Since *emercoarse_highlevelfilter4_outputhybird_bestsetup_bestfusion_lz* relies on pre-extracted faces. For convenience, we provide a more simple version that does not rely on pre-extracted face but can direct process on the raw frames.
```bash
AffectGPT
â”œâ”€â”€ models # Available at: https://pan.baidu.com/s/1IvC4H7Xt1AzMFocGMBBbHQ?pwd=hzf9
â”‚   â”œâ”€â”€ chinese-hubert-large # audio encoders
â”‚   â”œâ”€â”€ clip-vit-large-patch14 # video encoders
â”‚   â”œâ”€â”€ Qwen2.5-7B-Instruct # LLM
â”œâ”€â”€ output # Available at: https://pan.baidu.com/s/1iO-KyekHH3t7hDOVHy4ypg?pwd=yex1
â”‚   â”œâ”€â”€ mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz 
```

2. You can use any tool to extract audio and subtitle from video.
```bash
[1] generate ov labels
CUDA_VISIBLE_DEVICES=0 python -u inference_sample.py --zeroshot
--video_path='demo/sample_00000000.mp4' --audio_path='demo/sample_00000000.wav' --subtitle="I don't know! I, I, I don't have experience in this area."
--cfg-path=train_configs/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz.yaml 
--options "inference.test_epoch=30"

[2] generate emotion description
CUDA_VISIBLE_DEVICES=0 python -u inference_sample.py --zeroshot 
--video_path='demo/sample_00000000.mp4' --audio_path='demo/sample_00000000.wav' --subtitle="I don't know! I, I, I don't have experience in this area."
--outside_user_message="Please infer the person's emotional state and provide your reasoning process."
--cfg-path=train_configs/mercaptionplus_outputhybird_bestsetup_bestfusion_frame_lz.yaml 
--options "inference.test_epoch=30"

# For the above code, the output description should begin with "In the text", otherwise your inference code or downloaded model may contain errors.
```

## ğŸ“‘ Citation

If you find this project useful for your research and applications, please cite using this BibTeX:
```bibtex
@inproceedings{lian2025affectgpt,
  title={AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models},
  author={Lian, Zheng and Chen, Haoyu and Chen, Lan and Sun, Haiyang and Sun, Licai and Ren, Yong and Cheng, Zebang and Liu, Bin and Liu, Rui and Peng, Xiaojiang and others},
  booktitle={Proceedings of the 42nd International Conference on Machine Learning},
  year={2025}
}
```

## ğŸ‘ Acknowledgement
The codebase of AffectGPT is adapted from [**Video-LLaMA**](https://github.com/DAMO-NLP-SG/Video-LLaMA) and [**Video-LLaMA2**](https://github.com/DAMO-NLP-SG/VideoLLaMA2). We are also grateful for their excellent work.


## ğŸ”’ License

This project is released under the Apache 2.0 license as found in the LICENSE file. The service is a research preview intended for **non-commercial use ONLY**. Please get in touch with us if you find any potential violations.
